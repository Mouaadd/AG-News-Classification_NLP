{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Natural Language Processing (NLP) Project**\n",
        "\n",
        "**Option S2D** - **Ecole Centrale Casablanca** **March 24**\n",
        "\n",
        "\n",
        "> Hiba CHARKANI EL HASSANI\n",
        "\n",
        "> Badreddine BELACHKAR\n",
        "\n",
        "> Mouad KHAZNAOUI\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "9foBPUU4R8ce"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ce notebook détaille l'implémentation de la classification de texte en utilisant l'ensemble de données AG News et un modèle de classification BERT.\n",
        "\n",
        "## Préparation des dépendances\n",
        "\n",
        "Nous commençons par installer les dépendances nécessaires, notamment le package `portalocker` pour la gestion des verrous de fichiers et le package `tokenizers` pour le traitement des tokens."
      ],
      "metadata": {
        "id": "-0qsZA9qPnCc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "JXukEI5q_Xbj"
      },
      "outputs": [],
      "source": [
        "!pip install portalocker>=2.0.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qqMTFdrdLpgt",
        "outputId": "cd0f2f0a-e6d5-4851-e93a-32877332bada"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tokenizers in /usr/local/lib/python3.10/dist-packages (0.15.2)\n",
            "Requirement already satisfied: huggingface_hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from tokenizers) (0.20.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers) (3.13.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers) (2023.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers) (4.66.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers) (4.10.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers) (23.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub<1.0,>=0.16.4->tokenizers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub<1.0,>=0.16.4->tokenizers) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub<1.0,>=0.16.4->tokenizers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub<1.0,>=0.16.4->tokenizers) (2024.2.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install tokenizers"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chargement des données\n",
        "\n",
        "Ensuite, nous utilisons le module `torchtext.datasets` pour charger l'ensemble de données AG News. Nous créons un itérateur pour parcourir les exemples d'entraînement et récupérer le premier exemple pour l'inspection."
      ],
      "metadata": {
        "id": "Ugmc-pvoP8Dp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WCgLP-ru_Y6f",
        "outputId": "17e20568-cdcd-4dc3-b8cc-9a165cbfb24d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(3, \"Wall St. Bears Claw Back Into the Black (Reuters) Reuters - Short-sellers, Wall Street's dwindling\\\\band of ultra-cynics, are seeing green again.\")\n"
          ]
        }
      ],
      "source": [
        "from torchtext.datasets import AG_NEWS\n",
        "\n",
        "# Crée un itérateur\n",
        "train_iter = iter(AG_NEWS(split='train'))\n",
        "\n",
        "# Récupère le premier exemple\n",
        "first_example = next(train_iter)\n",
        "\n",
        "print(first_example)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Exploration de l'ensemble de données\n",
        "\n",
        "Nous visualisons ensuite la distribution des catégories dans l'ensemble de données en utilisant Matplotlib, en prenant les 1000 premiers exemples pour des raisons de performance. Nous calculons également la longueur moyenne des textes et affichons un exemple de texte pour chaque catégorie."
      ],
      "metadata": {
        "id": "JxAjVGE1QQQN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from collections import Counter\n",
        "\n",
        "\n",
        "# Récupère les exemples\n",
        "examples = []\n",
        "for example in train_iter:\n",
        "    examples.append(example)\n",
        "    if len(examples) >= 1000:  # Prend les 1000 premiers exemples pour des raisons de performance\n",
        "        break\n",
        "\n",
        "# Distribution des catégories\n",
        "categories = [example[0] for example in examples]\n",
        "category_counts = Counter(categories)\n",
        "plt.bar(category_counts.keys(), category_counts.values())\n",
        "plt.title('Distribution des catégories')\n",
        "plt.xlabel('Catégorie')\n",
        "plt.ylabel(\"Nombre d'exemples\")\n",
        "plt.show()\n",
        "\n",
        "# Longueur moyenne des textes\n",
        "text_lengths = [len(example[1].split()) for example in examples]\n",
        "average_length = sum(text_lengths) / len(text_lengths)\n",
        "print(f\"Longueur moyenne des textes : {average_length} mots\")\n",
        "\n",
        "# Exemple de texte pour chaque catégorie\n",
        "example_texts = {}\n",
        "for example in examples:\n",
        "    category = example[0]\n",
        "    if category not in example_texts:\n",
        "        example_texts[category] = example[1]\n",
        "        print(f\"Exemple de texte pour la catégorie {category}:\")\n",
        "        print(example[1])\n",
        "        print()\n",
        "\n",
        "# Catégorie avec le plus grand nombre d'exemples\n",
        "most_common_category = category_counts.most_common(1)[0][0]\n",
        "print(f\"Catégorie avec le plus grand nombre d'exemples : {most_common_category}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 736
        },
        "id": "gW5-wExAKd4Q",
        "outputId": "8892c4c0-9e85-4ff1-89fa-066696df048c"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjsAAAHICAYAAACoOCtxAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAABBOElEQVR4nO3deVRV9f7/8dcBBRQF1AJEEU3UnE3NQutrJUaK01Wzul4lcyhDTW3SJtNKU8upnMquQ8O1NLWr5ZRjGjmTpGiaY8qQeQXFBIXP749+nNURUA4cRHbPx1pnLfdnf/be78/ZsHi592efYzPGGAEAAFiUW3EXAAAAUJQIOwAAwNIIOwAAwNIIOwAAwNIIOwAAwNIIOwAAwNIIOwAAwNIIOwAAwNIIOwBQwqWkpGjMmDH67rvvirsU4KZE2AGK0Ouvvy6bzXZDjnXffffpvvvusy9v3LhRNptNixcvviHHf/zxx1W9evUbcqz8yB7/xo0bi7uUIte3b1+tXLlSzZs3L9Y6qlevrscff7xYawByQ9gB8mnevHmy2Wz2l5eXl4KCghQREaFp06bp/PnzLjnO6dOn9frrrys2NtYl+3Olm7k2q8nvez116lTt3btXy5cvV5kyZW5McUAJU6q4CwBKmjFjxqhGjRq6fPmyEhMTtXHjRg0dOlSTJk3Sf//7XzVq1Mje95VXXtGIESOc2v/p06c1evRoVa9eXU2aNMn3dmvWrHHqOAVxrdo+/PBDZWVlFXkNfxf5+TnIyMhQWlqaVq1apVtuueXGFpiLgwcPys2N/0Pj5kPYAZzUrl07h9sFI0eO1Pr169WhQwd16tRJ8fHx9v9hlypVSqVKFe2v2cWLF1W2bFl5eHgU6XGup3Tp0sV6/L8jDw8PvfTSS8VagzFGly5dUpkyZeTp6VmstQB5IYIDLvDAAw/o1Vdf1fHjx/XJJ5/Y23Obs7N27Vrdc8898vPzU7ly5VSnTh37H6yNGzfqzjvvlCT16dPHfsts3rx5kv6cl9OgQQPt2rVL//d//6eyZcvat716zk62zMxMvfTSSwoMDJS3t7c6deqkkydPOvTJa67FX/d5vdpym7OTlpamZ599VsHBwfL09FSdOnX0zjvvyBjj0M9ms2nQoEFatmyZGjRoIE9PT9WvX1+rVq3K/Q2/yq+//qouXbrI29tb/v7+GjZsmNLT03Ptu23bNj300EPy9fVV2bJl1bp1a23dutWhz/nz5zV06FBVr15dnp6e8vf3V9u2bbV79+7r1nLq1Cn17dtXQUFB8vT0VI0aNTRw4EBlZGRIks6ePavnnntODRs2VLly5eTj46N27drpxx9/tO/jeu91fseRva/mzZvLy8tLNWvW1OzZs3P9ubxy5YreeOMN1axZU56enqpevbpeeumlHO9j9erV1aFDB61evVrNmzdXmTJlNHv2bPu6q3+Ozp07p6FDh9p/BkJDQzV+/PgcVwEXLlyoZs2aqXz58vLx8VHDhg01derU677fQH5wZQdwkV69eumll17SmjVr1L9//1z77Nu3Tx06dFCjRo00ZswYeXp66vDhw/Y/UnXr1tWYMWP02muvacCAAbr33nslSS1btrTv4/fff1e7du306KOP6l//+pcCAgKuWddbb70lm82mF198UcnJyZoyZYrCw8MVGxvr1ByP/NT2V8YYderUSRs2bFDfvn3VpEkTrV69Ws8//7xOnTqlyZMnO/TfsmWLlixZoqefflrly5fXtGnT1K1bN504cUKVKlXKs64//vhDbdq00YkTJzRkyBAFBQXp448/1vr163P0Xb9+vdq1a6dmzZpp1KhRcnNz09y5c/XAAw/ou+++U4sWLSRJTz31lBYvXqxBgwapXr16+v3337VlyxbFx8eradOmedZy+vRptWjRQufOndOAAQN0++2369SpU1q8eLEuXrwoDw8PHTlyRMuWLdPDDz+sGjVqKCkpSbNnz1br1q21f/9+BQUFXfe9zu849uzZo4ceekiVK1fW6NGjlZmZqTFjxujWW2/NUXu/fv00f/58de/eXc8++6y2bdumcePGKT4+XkuXLnXoe/DgQT322GN68skn1b9/f9WpUyfX9+PixYtq3bq1Tp06pSeffFLVqlXT999/r5EjRyohIUFTpkyR9Od/AB577DG1adNG48ePlyTFx8dr69ateuaZZ/J8v4F8MwDyZe7cuUaS2bFjR559fH19zR133GFfHjVqlPnrr9nkyZONJPPbb7/luY8dO3YYSWbu3Lk51rVu3dpIMrNmzcp1XevWre3LGzZsMJJMlSpVTGpqqr39iy++MJLM1KlT7W0hISEmKirquvu8Vm1RUVEmJCTEvrxs2TIjybz55psO/bp3725sNps5fPiwvU2S8fDwcGj78ccfjSTz3nvv5TjWX02ZMsVIMl988YW9LS0tzYSGhhpJZsOGDcYYY7KyskytWrVMRESEycrKsve9ePGiqVGjhmnbtq29zdfX10RHR1/zuLnp3bu3cXNzy/VnJPuYly5dMpmZmQ7rjh49ajw9Pc2YMWPsbXm9186Mo2PHjqZs2bLm1KlT9rZDhw6ZUqVKOfxcxsbGGkmmX79+Dsd67rnnjCSzfv16e1tISIiRZFatWpVjjFf/HL3xxhvG29vb/Pzzzw79RowYYdzd3c2JEyeMMcY888wzxsfHx1y5ciXHPgFX4DYW4ELlypW75lNZfn5+kqSvvvqqwJN5PT091adPn3z37927t8qXL29f7t69uypXrqxvvvmmQMfPr2+++Ubu7u4aMmSIQ/uzzz4rY4xWrlzp0B4eHq6aNWvalxs1aiQfHx8dOXLkusepXLmyunfvbm8rW7asBgwY4NAvNjZWhw4d0j//+U/9/vvvOnPmjM6cOaO0tDS1adNGmzdvtp8TPz8/bdu2TadPn873eLOysrRs2TJ17Ngx10fAs28beXp62ifxZmZm6vfff7ffzszPbbL8jiMzM1PffvutunTpoqCgIPv2oaGhateuncM+s38Whg8f7tD+7LPPSpK+/vprh/YaNWooIiLiurUuWrRI9957rypUqGCv88yZMwoPD1dmZqY2b94s6c/3Oy0tTWvXrr3uPoGC4DYW4EIXLlyQv79/nusfeeQRzZkzR/369dOIESPUpk0bde3aVd27d8/3UyxVqlRxajJyrVq1HJZtNptCQ0N17NixfO+jII4fP66goCCHoCX9eTsse/1fVatWLcc+KlSooP/973/XPU5oaGiOOShX31o5dOiQJCkqKirPfaWkpKhChQqaMGGCoqKiFBwcrGbNmql9+/bq3bu3brvttjy3/e2335SamqoGDRpcs96srCxNnTpVM2bM0NGjR5WZmWlfd63bdc6O49KlS/rjjz8UGhqaY/3VbcePH5ebm1uO9sDAQPn5+eU4VzVq1Lhundm17t27N9fbZpKUnJwsSXr66af1xRdfqF27dqpSpYoefPBB9ejRQw899FC+jgNcD2EHcJFff/1VKSkpuf5xyVamTBlt3rxZGzZs0Ndff61Vq1bp888/1wMPPKA1a9bI3d39uscpis9SyeuDDzMzM/NVkyvkdRxz1WTmgsq+ajNx4sQ8H+UuV66cJKlHjx669957tXTpUq1Zs0YTJ07U+PHjtWTJkhxXRZw1duxYvfrqq3riiSf0xhtvqGLFinJzc9PQoUPzdbUvv+O4dOmS07Xl9wMw8/szmJWVpbZt2+qFF17IdX3t2rUlSf7+/oqNjdXq1au1cuVKrVy5UnPnzlXv3r01f/78/BUPXANhB3CRjz/+WJKue3nfzc1Nbdq0UZs2bTRp0iSNHTtWL7/8sjZs2KDw8HCXf+Jy9pWAbMYYHT582OHzgCpUqKBz587l2Pb48eMOVzOcqS0kJETffvutzp8/73B158CBA/b1rhASEqKffvpJxhiH+g4ePOjQL/sWmY+Pj8LDw6+738qVK+vpp5/W008/reTkZDVt2lRvvfVWnmHn1ltvlY+Pj3766adr7nfx4sW6//779dFHHzm0nzt3zuGzcvJ6r/M7Dn9/f3l5eenw4cM51l3dFhISoqysLB06dMh+5U2SkpKSdO7cuQKfq5o1a+rChQv5er89PDzUsWNHdezYUVlZWXr66ac1e/Zsvfrqq9f8DwSQH8zZAVxg/fr1euONN1SjRg317Nkzz35nz57N0Zb9v/PsR3y9vb0lKdfwURALFixwmEe0ePFiJSQkOPzRrlmzpn744Qf749GStGLFihyPqDtTW/v27ZWZman333/foX3y5Mmy2WyFvkLy1+OcPn3a4WsxLl68qA8++MChX7NmzVSzZk298847unDhQo79/Pbbb5L+vJqVkpLisM7f319BQUF5Ps4u/Rliu3TpouXLl2vnzp051mdfoXJ3d89xtWrRokU6deqUQ1te73V+x+Hu7q7w8HAtW7bMYe7R4cOHc8yXat++vSTZn47KNmnSJElSZGRkrmO+nh49eigmJkarV6/Ose7cuXO6cuWKpD+fMPwrNzc3exi/1nsO5BdXdgAnrVy5UgcOHNCVK1eUlJSk9evXa+3atQoJCdF///tfeXl55bntmDFjtHnzZkVGRiokJETJycmaMWOGqlatqnvuuUfSn8HDz89Ps2bNUvny5eXt7a277ror3/MkrlaxYkXdc8896tOnj5KSkjRlyhSFhoY6PB7fr18/LV68WA899JB69OihX375RZ988onDhGFna+vYsaPuv/9+vfzyyzp27JgaN26sNWvW6KuvvtLQoUNz7Lug+vfvr/fff1+9e/fWrl27VLlyZX388ccqW7asQz83NzfNmTNH7dq1U/369dWnTx9VqVJFp06d0oYNG+Tj46Ply5fr/Pnzqlq1qrp3767GjRurXLly+vbbb7Vjxw69++6716xl7NixWrNmjVq3bq0BAwaobt26SkhI0KJFi7Rlyxb5+fmpQ4cOGjNmjPr06aOWLVsqLi5On376aY75QNd6r/MzDunPz3las2aNWrVqpYEDB9rDZ4MGDRy+hqJx48aKiorSBx98oHPnzql169bavn275s+fry5duuj+++8v0Ll5/vnn9d///lcdOnTQ448/rmbNmiktLU1xcXFavHixjh07pltuuUX9+vXT2bNn9cADD6hq1ao6fvy43nvvPTVp0sThShNQYMX5KBhQkmQ/ep798vDwMIGBgaZt27Zm6tSpDo93Z7v60fN169aZzp07m6CgIOPh4WGCgoLMY489luPR3K+++srUq1fP/ohw9uPHrVu3NvXr18+1vrwePf/Pf/5jRo4cafz9/U2ZMmVMZGSkOX78eI7t3333XVOlShXj6elpWrVqZXbu3Jljn9eq7epHz40x5vz582bYsGEmKCjIlC5d2tSqVctMnDjR4ZFpY/589Dy3R73zeiT+asePHzedOnUyZcuWNbfccot55plnzKpVqxwePc+2Z88e07VrV1OpUiXj6elpQkJCTI8ePcy6deuMMcakp6eb559/3jRu3NiUL1/eeHt7m8aNG5sZM2Zct47sWnr37m1uvfVW4+npaW677TYTHR1t0tPTjTF/Pnr+7LPPmsqVK5syZcqYVq1amZiYGKfe6/yMI9u6devMHXfcYTw8PEzNmjXNnDlzzLPPPmu8vLwc+l2+fNmMHj3a1KhRw5QuXdoEBwebkSNHmkuXLjn0CwkJMZGRkbmOPbfzdf78eTNy5EgTGhpqPDw8zC233GJatmxp3nnnHZORkWGMMWbx4sXmwQcfNP7+/sbDw8NUq1bNPPnkkyYhISFf7zlwPTZjXDT7DwBQInTp0kX79u3LMZ8LsCrm7ACAhf3xxx8Oy4cOHdI333yT61eLAFbFlR0AsLDKlSvr8ccf12233abjx49r5syZSk9P1549e3J8BhNgVUxQBgALe+ihh/Sf//xHiYmJ8vT0VFhYmMaOHUvQwd8KV3YAAIClMWcHAABYGmEHAABYGmEHAABYGhOU9eeX1Z0+fVrly5d3+fcSAQCAomGM0fnz5xUUFCQ3t7yv3xB2JJ0+fVrBwcHFXQYAACiAkydPqmrVqnmuJ+xI9m9kPnnypHx8fIq5GgAAkB+pqakKDg62/x3PC2FHst+68vHxIewAAFDCXG8KChOUAQCApRF2AACApRF2AACApRF2AACApRF2AACApRF2AACApRF2AACApRF2AACApRF2AACApRF2AACApRF2AACApRF2AACApRF2AACApRF2AACApZUq7gIAALgRqo/4urhL+Ns69nZksR6fKzsAAMDSCDsAAMDSCDsAAMDSCDsAAMDSCDsAAMDSCDsAAMDSCDsAAMDSCDsAAMDSCDsAAMDSCDsAAMDSCDsAAMDSCDsAAMDSCDsAAMDSCDsAAMDSCDsAAMDSCDsAAMDSCDsAAMDSCDsAAMDSCDsAAMDSCDsAAMDSCDsAAMDSCDsAAMDSCDsAAMDSCDsAAMDSCDsAAMDSCDsAAMDSCDsAAMDSCDsAAMDSCDsAAMDSCDsAAMDSCDsAAMDSCDsAAMDSCDsAAMDSCDsAAMDSCDsAAMDSCDsAAMDSCDsAAMDSCDsAAMDSCDsAAMDSCDsAAMDSCDsAAMDSCDsAAMDSCDsAAMDSCDsAAMDSCDsAAMDSCDsAAMDSCDsAAMDSCDsAAMDSCDsAAMDSbpqw8/bbb8tms2no0KH2tkuXLik6OlqVKlVSuXLl1K1bNyUlJTlsd+LECUVGRqps2bLy9/fX888/rytXrtzg6gEAwM3qpgg7O3bs0OzZs9WoUSOH9mHDhmn58uVatGiRNm3apNOnT6tr16729ZmZmYqMjFRGRoa+//57zZ8/X/PmzdNrr712o4cAAABuUsUedi5cuKCePXvqww8/VIUKFeztKSkp+uijjzRp0iQ98MADatasmebOnavvv/9eP/zwgyRpzZo12r9/vz755BM1adJE7dq10xtvvKHp06crIyOjuIYEAABuIsUedqKjoxUZGanw8HCH9l27duny5csO7bfffruqVaummJgYSVJMTIwaNmyogIAAe5+IiAilpqZq3759eR4zPT1dqampDi8AAGBNpYrz4AsXLtTu3bu1Y8eOHOsSExPl4eEhPz8/h/aAgAAlJiba+/w16GSvz16Xl3Hjxmn06NGFrB4AAJQExXZl5+TJk3rmmWf06aefysvL64Yee+TIkUpJSbG/Tp48eUOPDwAAbpxiCzu7du1ScnKymjZtqlKlSqlUqVLatGmTpk2bplKlSikgIEAZGRk6d+6cw3ZJSUkKDAyUJAUGBuZ4Oit7ObtPbjw9PeXj4+PwAgAA1lRsYadNmzaKi4tTbGys/dW8eXP17NnT/u/SpUtr3bp19m0OHjyoEydOKCwsTJIUFhamuLg4JScn2/usXbtWPj4+qlev3g0fEwAAuPkU25yd8uXLq0GDBg5t3t7eqlSpkr29b9++Gj58uCpWrCgfHx8NHjxYYWFhuvvuuyVJDz74oOrVq6devXppwoQJSkxM1CuvvKLo6Gh5enre8DEBAICbT7FOUL6eyZMny83NTd26dVN6eroiIiI0Y8YM+3p3d3etWLFCAwcOVFhYmLy9vRUVFaUxY8YUY9UAAOBmYjPGmOIuorilpqbK19dXKSkpzN8BAIuqPuLr4i7hb+vY25FFst/8/v0u9s/ZAQAAKEqEHQAAYGmEHQAAYGmEHQAAYGmEHQAAYGmEHQAAYGmEHQAAYGmEHQAAYGmEHQAAYGmEHQAAYGmEHQAAYGmEHQAAYGmEHQAAYGmEHQAAYGmEHQAAYGmEHQAAYGmEHQAAYGmEHQAAYGmEHQAAYGmEHQAAYGmEHQAAYGmEHQAAYGmEHQAAYGmEHQAAYGmEHQAAYGmEHQAAYGmEHQAAYGmEHQAAYGmEHQAAYGmEHQAAYGmEHQAAYGmEHQAAYGmEHQAAYGmEHQAAYGmEHQAAYGmEHQAAYGmEHQAAYGmEHQAAYGmFDjuZmZmKjY3V//73P1fUAwAA4FJOh52hQ4fqo48+kvRn0GndurWaNm2q4OBgbdy40dX1AQAAFIrTYWfx4sVq3LixJGn58uU6evSoDhw4oGHDhunll192eYEAAACF4XTYOXPmjAIDAyVJ33zzjR5++GHVrl1bTzzxhOLi4lxeIAAAQGE4HXYCAgK0f/9+ZWZmatWqVWrbtq0k6eLFi3J3d3d5gQAAAIVRytkN+vTpox49eqhy5cqy2WwKDw+XJG3btk233367ywsEAAAoDKfDzuuvv64GDRro5MmTevjhh+Xp6SlJcnd314gRI1xeIAAAQGE4HXYkqXv37pKkS5cu2duioqJcUxEAAIALOT1nJzMzU2+88YaqVKmicuXK6ciRI5KkV1991f5IOgAAwM3C6bDz1ltvad68eZowYYI8PDzs7Q0aNNCcOXNcWhwAAEBhOR12FixYoA8++EA9e/Z0ePqqcePGOnDggEuLAwAAKCynw86pU6cUGhqaoz0rK0uXL192SVEAAACu4nTYqVevnr777rsc7YsXL9Ydd9zhkqIAAABcxemnsV577TVFRUXp1KlTysrK0pIlS3Tw4EEtWLBAK1asKIoaAQAACszpKzudO3fW8uXL9e2338rb21uvvfaa4uPjtXz5cvunKQMAANwsCvQ5O/fee6/Wrl3r6loAAABczukrOwAAACVJvq7sVKhQQTabLV87PHv2bKEKAgAAcKV8hZ0pU6YUcRkAAABFI19hh++9AgAAJVWBJihnZmZq6dKlio+Pl/TnZ+907txZpUoVaHcAAABFxul0sm/fPnXq1EmJiYmqU6eOJGn8+PG69dZbtXz5cjVo0MDlRQIAABSU009j9evXT/Xr19evv/6q3bt3a/fu3Tp58qQaNWqkAQMGFEWNAAAABeb0lZ3Y2Fjt3LlTFSpUsLdVqFBBb731lu68806XFgcAAFBYTl/ZqV27tpKSknK0Jycn5/oFoQAAAMXJ6bAzbtw4DRkyRIsXL9avv/6qX3/9VYsXL9bQoUM1fvx4paam2l/XM3PmTDVq1Eg+Pj7y8fFRWFiYVq5caV9/6dIlRUdHq1KlSipXrpy6deuWI2idOHFCkZGRKlu2rPz9/fX888/rypUrzg4LAABYlNO3sTp06CBJ6tGjh/2DBo0xkqSOHTval202mzIzM6+5r6pVq+rtt99WrVq1ZIzR/Pnz1blzZ+3Zs0f169fXsGHD9PXXX2vRokXy9fXVoEGD1LVrV23dulXSn0+FRUZGKjAwUN9//70SEhLUu3dvlS5dWmPHjnV2aAAAwIJsJjup5NOmTZvy3bd169ZOF1SxYkVNnDhR3bt316233qrPPvtM3bt3lyQdOHBAdevWVUxMjO6++26tXLlSHTp00OnTpxUQECBJmjVrll588UX99ttv8vDwyNcxU1NT5evrq5SUFPn4+DhdMwDg5ld9xNfFXcLf1rG3I4tkv/n9++30lZ2CBJj8yMzM1KJFi5SWlqawsDDt2rVLly9fVnh4uL3P7bffrmrVqtnDTkxMjBo2bGgPOpIUERGhgQMHat++fbrjjjtyPVZ6errS09Pty/m55QYAAEqmAn0K4KVLl7R3714lJycrKyvLYV2nTp2c2ldcXJzCwsJ06dIllStXTkuXLlW9evUUGxsrDw8P+fn5OfQPCAhQYmKiJCkxMdEh6GSvz16Xl3Hjxmn06NFO1QkAAEomp8POqlWr1Lt3b505cybHuvzM07lanTp1FBsbq5SUFC1evFhRUVFO3SoriJEjR2r48OH25dTUVAUHBxfpMQEAQPFw+mmswYMH6+GHH1ZCQoKysrIcXs4GHUny8PBQaGiomjVrpnHjxqlx48aaOnWqAgMDlZGRoXPnzjn0T0pKUmBgoCQpMDAwx9NZ2cvZfXLj6elpfwIs+wUAAKzJ6bCTlJSk4cOH57h95CpZWVlKT09Xs2bNVLp0aa1bt86+7uDBgzpx4oTCwsIkSWFhYYqLi1NycrK9z9q1a+Xj46N69eoVSX0AAKBkcfo2Vvfu3bVx40bVrFmz0AcfOXKk2rVrp2rVqun8+fP67LPPtHHjRq1evVq+vr7q27evhg8frooVK8rHx0eDBw9WWFiY7r77bknSgw8+qHr16qlXr16aMGGCEhMT9corryg6Olqenp6Frg8AAJR8Toed999/Xw8//LC+++47NWzYUKVLl3ZYP2TIkHzvKzk5Wb1791ZCQoJ8fX3VqFEjrV69Wm3btpUkTZ48WW5uburWrZvS09MVERGhGTNm2Ld3d3fXihUrNHDgQIWFhcnb21tRUVEaM2aMs8MCAAAW5fTn7Hz00Ud66qmn5OXlpUqVKtk/WFD6c4LykSNHXF5kUeNzdgDA+vicneJT4j5n5+WXX9bo0aM1YsQIubk5PeUHAADghnI6rWRkZOiRRx4h6AAAgBLB6cQSFRWlzz//vChqAQAAcDmnb2NlZmZqwoQJWr16tRo1apRjgvKkSZNcVhwAAEBhOR124uLi7N859dNPPzms++tkZQAAgJuB02Fnw4YNRVEHAABAkSjwLOPDhw9r9erV+uOPPyRJTj7BDgAAcEM4HXZ+//13tWnTRrVr11b79u2VkJAgSerbt6+effZZlxcIAABQGE6HnWHDhql06dI6ceKEypYta29/5JFHtGrVKpcWBwAAUFhOz9lZs2aNVq9erapVqzq016pVS8ePH3dZYQAAAK7g9JWdtLQ0hys62c6ePcuXbwIAgJuO02Hn3nvv1YIFC+zLNptNWVlZmjBhgu6//36XFgcAAFBYTt/GmjBhgtq0aaOdO3cqIyNDL7zwgvbt26ezZ89q69atRVEjAABAgTl9ZadBgwb6+eefdc8996hz585KS0tT165dtWfPHtWsWbMoagQAACgwp6/sXLp0Sb6+vnr55ZdzrEtISFDlypVdUhgAAIArOH1lp2nTpoqNjc3R/uWXX6pRo0auqAkAAMBlnA479913n+6++26NHz9e0p9PZz3++OPq1auXXnrpJZcXCAAAUBhO38aaMWOGIiMj1a9fP61YsUIJCQkqV66ctm/frgYNGhRFjQAAAAXmdNiRpHbt2qlr166aOXOmSpUqpeXLlxN0AADATcnp21i//PKLwsLCtGLFCq1evVovvPCCOnXqpBdeeEGXL18uihoBAAAKzOmw06RJE9WoUUM//vij2rZtqzfffFMbNmzQkiVL1KJFi6KoEQAAoMCcDjszZszQwoUL5efnZ29r2bKl9uzZo6ZNm7qyNgAAgEJzOuz06tVLkpSRkaGDBw/qypUrkqTy5cvro48+cm11AAAAheR02Pnjjz/Ut29flS1bVvXr19eJEyckSYMHD7Y/jg4AAHCzcDrsjBgxQj/++KM2btwoLy8ve3t4eLgWLlzo0uIAAAAKy+lHz5ctW6bPP/9cd999t2w2m729fv36+uWXX1xaHAAAQGE5fWXnt99+k7+/f472tLQ0h/ADAABwM3A67DRv3lxff/21fTk74MyZM0dhYWGuqwwAAMAFnL6NNXbsWLVr10779+/XlStXNHXqVO3fv1/ff/+9Nm3aVBQ1AgAAFJjTV3buuecexcbG6sqVK2rYsKHWrFkjf39/xcTEqFmzZkVRIwAAQIEV6LuxatasqQ8//NDVtQAAALic01d2AAAASpJ8X9lxc3OTzWaTMUY2m02ZmZlFWRcAAIBL5DvsHD16tCjrAAAAKBL5DjshISFFWQcAAECRyFfY2bt3b7532KhRowIXAwAA4Gr5CjtNmjRxmK9zLczlAQAAN5N8PY119OhRHTlyREePHtWXX36pGjVqaMaMGdqzZ4/27NmjGTNmqGbNmvryyy+Lul4AAACn5OvKzl/n6zz88MOaNm2a2rdvb29r1KiRgoOD9eqrr6pLly4uLxIAAKCgnP6cnbi4ONWoUSNHe40aNbR//36XFAUAAOAqToedunXraty4ccrIyLC3ZWRkaNy4capbt65LiwMAACgsp78uYtasWerYsaOqVq1qf/Jq7969stlsWr58ucsLBAAAKAynw06LFi105MgRffrppzpw4IAk6ZFHHtE///lPeXt7u7xAAACAwijQF4F6e3trwIABrq4FAADA5fgiUAAAYGmEHQAAYGmEHQAAYGmEHQAAYGkFCjvnzp3TnDlzNHLkSJ09e1aStHv3bp06dcqlxQEAABSW009j7d27V+Hh4fL19dWxY8fUv39/VaxYUUuWLNGJEye0YMGCoqgTAACgQJy+sjN8+HA9/vjjOnTokLy8vOzt7du31+bNm11aHAAAQGE5HXZ27NihJ598Mkd7lSpVlJiY6JKiAAAAXMXpsOPp6anU1NQc7T///LNuvfVWlxQFAADgKk6HnU6dOmnMmDG6fPmyJMlms+nEiRN68cUX1a1bN5cXCAAAUBhOh513331XFy5ckL+/v/744w+1bt1aoaGhKl++vN56662iqBEAAKDAnH4ay9fXV2vXrtXWrVv1448/6sKFC2ratKnCw8OLoj4AAIBCcSrsXL58WWXKlFFsbKxatWqlVq1aFVVdAAAALuHUbazSpUurWrVqyszMLKp6AAAAXMrpOTsvv/yyXnrpJfsnJwMAANzMnJ6z8/777+vw4cMKCgpSSEiIvL29Hdbv3r3bZcVZQfURXxd3CX9bx96OLO4SAAA3AafDTpcuXYqgDAAAgKLhdNgZNWpUUdQBAABQJJwOO9l27typ+Ph4SVK9evXUrFkzlxUFAADgKk6HnV9//VWPPfaYtm7dKj8/P0nSuXPn1LJlSy1cuFBVq1Z1dY0AAAAF5vTTWP369dPly5cVHx+vs2fP6uzZs4qPj1dWVpb69evn1L7GjRunO++8U+XLl5e/v7+6dOmigwcPOvS5dOmSoqOjValSJZUrV07dunVTUlKSQ58TJ04oMjJSZcuWlb+/v55//nlduXLF2aEBAAALcjrsbNq0STNnzlSdOnXsbXXq1NF7772nzZs3O72v6Oho/fDDD1q7dq0uX76sBx98UGlpafY+w4YN0/Lly7Vo0SJt2rRJp0+fVteuXe3rMzMzFRkZqYyMDH3//feaP3++5s2bp9dee83ZoQEAAAty+jZWcHCw/UtA/yozM1NBQUFO7WvVqlUOy/PmzZO/v7927dql//u//1NKSoo++ugjffbZZ3rggQckSXPnzlXdunX1ww8/6O6779aaNWu0f/9+ffvttwoICFCTJk30xhtv6MUXX9Trr78uDw8PZ4cIAAAsxOkrOxMnTtTgwYO1c+dOe9vOnTv1zDPP6J133ilUMSkpKZKkihUrSpJ27dqly5cvO3zv1u23365q1aopJiZGkhQTE6OGDRsqICDA3iciIkKpqanat29frsdJT09XamqqwwsAAFhTvq7sVKhQQTabzb6clpamu+66S6VK/bn5lStXVKpUKT3xxBMF/hyerKwsDR06VK1atVKDBg0kSYmJifLw8LBPhM4WEBCgxMREe5+/Bp3s9dnrcjNu3DiNHj26QHUCAICSJV9hZ8qUKUVchhQdHa2ffvpJW7ZsKfJjjRw5UsOHD7cvp6amKjg4uMiPCwAAbrx8hZ2oqKgiLWLQoEFasWKFNm/e7PDoemBgoDIyMnTu3DmHqztJSUkKDAy099m+fbvD/rKf1sruczVPT095enq6eBQAAOBm5PScnWzJycn66aeftHfvXoeXM4wxGjRokJYuXar169erRo0aDuubNWum0qVLa926dfa2gwcP6sSJEwoLC5MkhYWFKS4uTsnJyfY+a9eulY+Pj+rVq1fQ4QEAAItw+mmsXbt2KSoqSvHx8TLGOKyz2WzKzMzM976io6P12Wef6auvvlL58uXtc2x8fX1VpkwZ+fr6qm/fvho+fLgqVqwoHx8fDR48WGFhYbr77rslSQ8++KDq1aunXr16acKECUpMTNQrr7yi6Ohort4AAADnw84TTzyh2rVr66OPPlJAQIDDxGVnzZw5U5J03333ObTPnTtXjz/+uCRp8uTJcnNzU7du3ZSenq6IiAjNmDHD3tfd3V0rVqzQwIEDFRYWJm9vb0VFRWnMmDEFrgsAAFiH02HnyJEj+vLLLxUaGlrog199ZSg3Xl5emj59uqZPn55nn5CQEH3zzTeFrgcAAFiP03N22rRpox9//LEoagEAAHA5p6/szJkzR1FRUfrpp5/UoEEDlS5d2mF9p06dXFYcAABAYTkddmJiYrR161atXLkyxzpnJygDAAAUNadvYw0ePFj/+te/lJCQoKysLIcXQQcAANxsnA47v//+u4YNG5bjKxoAAABuRk6Hna5du2rDhg1FUQsAAIDLOT1np3bt2ho5cqS2bNmihg0b5pigPGTIEJcVBwAAUFgFehqrXLly2rRpkzZt2uSwzmazEXYAAMBNxemwc/To0aKoAwAAoEgU+ItApT8/ATk/n4IMAABQXAoUdhYsWKCGDRuqTJkyKlOmjBo1aqSPP/7Y1bUBAAAUmtO3sSZNmqRXX31VgwYNUqtWrSRJW7Zs0VNPPaUzZ85o2LBhLi8SAACgoJwOO++9955mzpyp3r1729s6deqk+vXr6/XXXyfsACjxqo/4urhL+Fs69nZkcZcAi3L6NlZCQoJatmyZo71ly5ZKSEhwSVEAAACu4nTYCQ0N1RdffJGj/fPPP1etWrVcUhQAAICrOH0ba/To0XrkkUe0efNm+5ydrVu3at26dbmGIAAAgOLk9JWdbt26adu2bbrlllu0bNkyLVu2TLfccou2b9+uf/zjH0VRIwAAQIE5fWVHkpo1a6ZPPvnE1bUAAAC4XKE+VBAAAOBml+8rO25ubrLZbNfsY7PZdOXKlUIXBQAA4Cr5DjtLly7Nc11MTIymTZumrKwslxQFAADgKvkOO507d87RdvDgQY0YMULLly9Xz549NWbMGJcWBwAAUFgFmrNz+vRp9e/fXw0bNtSVK1cUGxur+fPnKyQkxNX1AQAAFIpTYSclJUUvvviiQkNDtW/fPq1bt07Lly9XgwYNiqo+AACAQsn3bawJEyZo/PjxCgwM1H/+859cb2sBAADcbPIddkaMGKEyZcooNDRU8+fP1/z583Ptt2TJEpcVBwAAUFj5Dju9e/e+7qPnAAAAN5t8h5158+YVYRkAAABFg09QBgAAlkbYAQAAlkbYAQAAlkbYAQAAlkbYAQAAlkbYAQAAlkbYAQAAlkbYAQAAlpbvDxUE4Kj6iK+Lu4S/rWNvRxZ3CQBKEK7sAAAASyPsAAAASyPsAAAASyPsAAAASyPsAAAASyPsAAAASyPsAAAASyPsAAAASyPsAAAASyPsAAAASyPsAAAASyPsAAAASyPsAAAASyPsAAAASyPsAAAASyPsAAAASyPsAAAASyPsAAAASyPsAAAASyPsAAAASyPsAAAASyPsAAAASyPsAAAASyPsAAAASyPsAAAASyPsAAAASyvWsLN582Z17NhRQUFBstlsWrZsmcN6Y4xee+01Va5cWWXKlFF4eLgOHTrk0Ofs2bPq2bOnfHx85Ofnp759++rChQs3cBQAAOBmVqxhJy0tTY0bN9b06dNzXT9hwgRNmzZNs2bN0rZt2+Tt7a2IiAhdunTJ3qdnz57at2+f1q5dqxUrVmjz5s0aMGDAjRoCAAC4yZUqzoO3a9dO7dq1y3WdMUZTpkzRK6+8os6dO0uSFixYoICAAC1btkyPPvqo4uPjtWrVKu3YsUPNmzeXJL333ntq37693nnnHQUFBd2wsQAAgJvTTTtn5+jRo0pMTFR4eLi9zdfXV3fddZdiYmIkSTExMfLz87MHHUkKDw+Xm5ubtm3bdsNrBgAAN59ivbJzLYmJiZKkgIAAh/aAgAD7usTERPn7+zusL1WqlCpWrGjvk5v09HSlp6fbl1NTU11VNgAAuMnctFd2itK4cePk6+trfwUHBxd3SQAAoIjctGEnMDBQkpSUlOTQnpSUZF8XGBio5ORkh/VXrlzR2bNn7X1yM3LkSKWkpNhfJ0+edHH1AADgZnHThp0aNWooMDBQ69ats7elpqZq27ZtCgsLkySFhYXp3Llz2rVrl73P+vXrlZWVpbvuuivPfXt6esrHx8fhBQAArKlY5+xcuHBBhw8fti8fPXpUsbGxqlixoqpVq6ahQ4fqzTffVK1atVSjRg29+uqrCgoKUpcuXSRJdevW1UMPPaT+/ftr1qxZunz5sgYNGqRHH32UJ7EAAICkYg47O3fu1P33329fHj58uCQpKipK8+bN0wsvvKC0tDQNGDBA586d0z333KNVq1bJy8vLvs2nn36qQYMGqU2bNnJzc1O3bt00bdq0Gz4WAABwcyrWsHPffffJGJPnepvNpjFjxmjMmDF59qlYsaI+++yzoigPAABYwE07ZwcAAMAVCDsAAMDSCDsAAMDSCDsAAMDSCDsAAMDSCDsAAMDSCDsAAMDSCDsAAMDSCDsAAMDSCDsAAMDSCDsAAMDSCDsAAMDSCDsAAMDSCDsAAMDSCDsAAMDSCDsAAMDSCDsAAMDSCDsAAMDSCDsAAMDSCDsAAMDSCDsAAMDSCDsAAMDSCDsAAMDSCDsAAMDSCDsAAMDSCDsAAMDSCDsAAMDSCDsAAMDSCDsAAMDSCDsAAMDSCDsAAMDSCDsAAMDSCDsAAMDSCDsAAMDSCDsAAMDSCDsAAMDSCDsAAMDSCDsAAMDSCDsAAMDSCDsAAMDSCDsAAMDSCDsAAMDSCDsAAMDSCDsAAMDSCDsAAMDSCDsAAMDSCDsAAMDSCDsAAMDSCDsAAMDSCDsAAMDSCDsAAMDSCDsAAMDSCDsAAMDSCDsAAMDSCDsAAMDSCDsAAMDSCDsAAMDSCDsAAMDSCDsAAMDSCDsAAMDSCDsAAMDSCDsAAMDSCDsAAMDSCDsAAMDSLBN2pk+frurVq8vLy0t33XWXtm/fXtwlAQCAm4Alws7nn3+u4cOHa9SoUdq9e7caN26siIgIJScnF3dpAACgmFki7EyaNEn9+/dXnz59VK9ePc2aNUtly5bVv//97+IuDQAAFLNSxV1AYWVkZGjXrl0aOXKkvc3NzU3h4eGKiYnJdZv09HSlp6fbl1NSUiRJqampLq8vK/2iy/eJ/CmK8/lXnNviw7m1Js6rdRXVuc3erzHmmv1KfNg5c+aMMjMzFRAQ4NAeEBCgAwcO5LrNuHHjNHr06BztwcHBRVIjiofvlOKuAEWFc2tNnFfrKupze/78efn6+ua5vsSHnYIYOXKkhg8fbl/OysrS2bNnValSJdlstjy3S01NVXBwsE6ePCkfH58bUWqx+juNl7Fa199pvIzVuv5O43VmrMYYnT9/XkFBQdfsV+LDzi233CJ3d3clJSU5tCclJSkwMDDXbTw9PeXp6enQ5ufnl+9j+vj4WP6H7a/+TuNlrNb1dxovY7Wuv9N48zvWa13RyVbiJyh7eHioWbNmWrdunb0tKytL69atU1hYWDFWBgAAbgYl/sqOJA0fPlxRUVFq3ry5WrRooSlTpigtLU19+vQp7tIAAEAxs0TYeeSRR/Tbb7/ptddeU2Jiopo0aaJVq1blmLRcWJ6enho1alSOW2BW9XcaL2O1rr/TeBmrdf2dxlsUY7WZ6z2vBQAAUIKV+Dk7AAAA10LYAQAAlkbYAQAAlkbYAQAAlkbYucr06dNVvXp1eXl56a677tL27dvz7Dtv3jzZbDaHl5eX1w2stuA2b96sjh07KigoSDabTcuWLbvuNhs3blTTpk3l6emp0NBQzZs3r8jrdBVnx7tx48Yc59ZmsykxMfHGFFxA48aN05133qny5cvL399fXbp00cGDB6+73aJFi3T77bfLy8tLDRs21DfffHMDqi28goy3pP7ezpw5U40aNbJ/0FpYWJhWrlx5zW1K6nl1dqwl9Zzm5u2335bNZtPQoUOv2a+kntur5We8rji/hJ2/+PzzzzV8+HCNGjVKu3fvVuPGjRUREaHk5OQ8t/Hx8VFCQoL9dfz48RtYccGlpaWpcePGmj59er76Hz16VJGRkbr//vsVGxuroUOHql+/flq9enURV+oazo4328GDBx3Or7+/fxFV6BqbNm1SdHS0fvjhB61du1aXL1/Wgw8+qLS0tDy3+f777/XYY4+pb9++2rNnj7p06aIuXbrop59+uoGVF0xBxiuVzN/bqlWr6u2339auXbu0c+dOPfDAA+rcubP27duXa/+SfF6dHatUMs/p1Xbs2KHZs2erUaNG1+xXks/tX+V3vJILzq+BXYsWLUx0dLR9OTMz0wQFBZlx48bl2n/u3LnG19f3BlVXdCSZpUuXXrPPCy+8YOrXr+/Q9sgjj5iIiIgirKxo5Ge8GzZsMJLM//73vxtSU1FJTk42ksymTZvy7NOjRw8TGRnp0HbXXXeZJ598sqjLc7n8jNcqv7fGGFOhQgUzZ86cXNdZ6bwac+2xWuGcnj9/3tSqVcusXbvWtG7d2jzzzDN59rXCuXVmvK44v1zZ+f8yMjK0a9cuhYeH29vc3NwUHh6umJiYPLe7cOGCQkJCFBwcfN3/eZRkMTExDu+NJEVERFzzvbGCJk2aqHLlymrbtq22bt1a3OU4LSUlRZJUsWLFPPtY6dzmZ7xSyf+9zczM1MKFC5WWlpbn1+JY5bzmZ6xSyT+n0dHRioyMzHHOcmOFc+vMeKXCn1/Czv935swZZWZm5vjU5YCAgDznadSpU0f//ve/9dVXX+mTTz5RVlaWWrZsqV9//fVGlHxDJSYm5vrepKam6o8//iimqopO5cqVNWvWLH355Zf68ssvFRwcrPvuu0+7d+8u7tLyLSsrS0OHDlWrVq3UoEGDPPvldW5v9vlJV8vveEvy721cXJzKlSsnT09PPfXUU1q6dKnq1auXa9+Sfl6dGWtJPqeStHDhQu3evVvjxo3LV/+Sfm6dHa8rzq8lvi6iuISFhTn8T6Nly5aqW7euZs+erTfeeKMYK0Nh1alTR3Xq1LEvt2zZUr/88osmT56sjz/+uBgry7/o6Gj99NNP2rJlS3GXckPkd7wl+fe2Tp06io2NVUpKihYvXqyoqCht2rQpzxBQkjkz1pJ8Tk+ePKlnnnlGa9euLbGTqp1RkPG64vwSdv6/W265Re7u7kpKSnJoT0pKUmBgYL72Ubp0ad1xxx06fPhwUZRYrAIDA3N9b3x8fFSmTJliqurGatGiRYkJDoMGDdKKFSu0efNmVa1a9Zp98zq3+f25vxk4M96rlaTfWw8PD4WGhkqSmjVrph07dmjq1KmaPXt2jr4l/bw6M9arlaRzumvXLiUnJ6tp06b2tszMTG3evFnvv/++0tPT5e7u7rBNST63BRnv1QpyfrmN9f95eHioWbNmWrdunb0tKytL69atu+Z94r/KzMxUXFycKleuXFRlFpuwsDCH90aS1q5dm+/3xgpiY2Nv+nNrjNGgQYO0dOlSrV+/XjVq1LjuNiX53BZkvFcryb+3WVlZSk9Pz3VdST6vubnWWK9Wks5pmzZtFBcXp9jYWPurefPm6tmzp2JjY3P9w1+Sz21Bxnu1Ap3fQk1vtpiFCxcaT09PM2/ePLN//34zYMAA4+fnZxITE40xxvTq1cuMGDHC3n/06NFm9erV5pdffjG7du0yjz76qPHy8jL79u0rriHk2/nz582ePXvMnj17jCQzadIks2fPHnP8+HFjjDEjRowwvXr1svc/cuSIKVu2rHn++edNfHy8mT59unF3dzerVq0qriE4xdnxTp482SxbtswcOnTIxMXFmWeeeca4ubmZb7/9triGkC8DBw40vr6+ZuPGjSYhIcH+unjxor3P1T/HW7duNaVKlTLvvPOOiY+PN6NGjTKlS5c2cXFxxTEEpxRkvCX193bEiBFm06ZN5ujRo2bv3r1mxIgRxmazmTVr1hhjrHVenR1rST2nebn66SQrndvcXG+8rji/hJ2rvPfee6ZatWrGw8PDtGjRwvzwww/2da1btzZRUVH25aFDh9r7BgQEmPbt25vdu3cXQ9XOy360+upX9viioqJM69atc2zTpEkT4+HhYW677TYzd+7cG153QTk73vHjx5uaNWsaLy8vU7FiRXPfffeZ9evXF0/xTshtjJIcztXVP8fGGPPFF1+Y2rVrGw8PD1O/fn3z9ddf39jCC6gg4y2pv7dPPPGECQkJMR4eHubWW281bdq0sf/xN8Za59XZsZbUc5qXq//4W+nc5uZ643XF+bUZY0z+rwMBAACULMzZAQAAlkbYAQAAlkbYAQAAlkbYAQAAlkbYAQAAlkbYAQAAlkbYAQAAlkbYAWAJGRkZGjt2rOLj44u7FAA3GcIOAEt49tlnFRcXp9tvv/2GHG/jxo2y2Ww6d+7cDTkegIIj7AAocomJiRo8eLBuu+02eXp6Kjg4WB07dszxZYZ5mTdvnvz8/PJc/8UXX2jfvn2aP3++bDabi6q+tpYtWyohIUG+vr435HgACq5UcRcAwNqOHTumVq1ayc/PTxMnTlTDhg11+fJlrV69WtHR0Tpw4EChj9GjRw/16NHDBdXmz+XLl+Xh4aHAwMAbdkwABceVHQBF6umnn5bNZtP27dvVrVs31a5dW/Xr19fw4cP1ww8/SJImTZqkhg0bytvbW8HBwXr66ad14cIFSX/eLurTp49SUlJks9lks9n0+uuvS5LS09P13HPPqUqVKvL29tZdd92ljRs3Ohz/ww8/VHBwsMqWLat//OMfmjRpUo6rRDNnzlTNmjXl4eGhOnXq6OOPP3ZYb7PZNHPmTHXq1Ene3t566623cr2NtWXLFt17770qU6aMgoODNWTIEKWlpbn0/QRQAC75ylIAyMXvv/9ubDabGTt27DX7TZ482axfv94cPXrUrFu3ztSpU8cMHDjQGGNMenq6mTJlivHx8TEJCQkmISHBnD9/3hhjTL9+/UzLli3N5s2bzeHDh83EiRONp6en+fnnn40xxmzZssW4ubmZiRMnmoMHD5rp06ebihUrGl9fX/uxlyxZYkqXLm2mT59uDh48aN59913j7u7u8C33koy/v7/597//bX755Rdz/Phxs2HDBiPJ/O9//zPGGHP48GHj7e1tJk+ebH7++WezdetWc8cdd5jHH3/che8ogIIg7AAoMtu2bTOSzJIlS5zabtGiRaZSpUr25blz5zoEFGOMOX78uHF3dzenTp1yaG/Tpo0ZOXKkMcaYRx55xERGRjqs79mzp8O+WrZsafr37+/Q5+GHHzbt27e3L0syQ4cOdehzddjp27evGTBggEOf7777zri5uZk//vjj+oMGUGS4jQWgyBhj8tXv22+/VZs2bVSlShWVL19evXr10u+//66LFy/muU1cXJwyMzNVu3ZtlStXzv7atGmTfvnlF0nSwYMH1aJFC4ftrl6Oj49Xq1atHNpatWqV4xH25s2bX3MMP/74o+bNm+dQS0REhLKysnT06NHrvgcAig4TlAEUmVq1aslms11zEvKxY8fUoUMHDRw4UG+99ZYqVqyoLVu2qG/fvsrIyFDZsmVz3e7ChQtyd3fXrl275O7u7rCuXLlyLh2HJHl7e19z/YULF/Tkk09qyJAhOdZVq1bN5fUAyD/CDoAiU7FiRUVERGj69OkaMmRIjsBw7tw57dq1S1lZWXr33Xfl5vbnxeYvvvjCoZ+Hh4cyMzMd2u644w5lZmYqOTlZ9957b67Hr1Onjnbs2OHQdvVy3bp1tXXrVkVFRdnbtm7dqnr16jk11qZNm2r//v0KDQ11ajsARY/bWACK1PTp05WZmakWLVroyy+/1KFDhxQfH69p06YpLCxMoaGhunz5st577z0dOXJEH3/8sWbNmuWwj+rVq+vChQtat26dzpw5o4sXL6p27drq2bOnevfurSVLlujo0aPavn27xo0bp6+//lqSNHjwYH3zzTeaNGmSDh06pNmzZ2vlypUOn8Xz/PPPa968eZo5c6YOHTqkSZMmacmSJXruueecGueLL76o77//XoMGDVJsbKwOHTqkr776SoMGDSr8mwigcIp70hAA6zt9+rSJjo42ISEhxsPDw1SpUsV06tTJbNiwwRhjzKRJk0zlypVNmTJlTEREhFmwYIHD5F9jjHnqqadMpUqVjCQzatQoY4wxGRkZ5rXXXjPVq1c3pUuXNpUrVzb/+Mc/zN69e+3bffDBB6ZKlSqmTJkypkuXLubNN980gYGBDvXNmDHD3HbbbaZ06dKmdu3aZsGCBQ7rJZmlS5c6tF09QdkYY7Zv327atm1rypUrZ7y9vU2jRo3MW2+9Vej3D0Dh2IzJ5wxCALCA/v3768CBA/ruu++KuxQANwhzdgBY2jvvvKO2bdvK29tbK1eu1Pz58zVjxoziLgvADcSVHQCW1qNHD23cuFHnz5/XbbfdpsGDB+upp54q7rIA3ECEHQAAYGk8jQUAACyNsAMAACyNsAMAACyNsAMAACyNsAMAACyNsAMAACyNsAMAACyNsAMAACyNsAMAACzt/wHAsxWQAKgKZgAAAABJRU5ErkJggg==\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Longueur moyenne des textes : 39.905 mots\n",
            "Exemple de texte pour la catégorie 3:\n",
            "Wall St. Bears Claw Back Into the Black (Reuters) Reuters - Short-sellers, Wall Street's dwindling\\band of ultra-cynics, are seeing green again.\n",
            "\n",
            "Exemple de texte pour la catégorie 4:\n",
            "'Madden,' 'ESPN' Football Score in Different Ways (Reuters) Reuters - Was absenteeism a little high\\on Tuesday among the guys at the office? EA Sports would like\\to think it was because \"Madden NFL 2005\" came out that day,\\and some fans of the football simulation are rabid enough to\\take a sick day to play it.\n",
            "\n",
            "Exemple de texte pour la catégorie 2:\n",
            "Phelps, Thorpe Advance in 200 Freestyle (AP) AP - Michael Phelps took care of qualifying for the Olympic 200-meter freestyle semifinals Sunday, and then found out he had been added to the American team for the evening's 400 freestyle relay final. Phelps' rivals Ian Thorpe and Pieter van den Hoogenband and teammate Klete Keller were faster than the teenager in the 200 free preliminaries.\n",
            "\n",
            "Exemple de texte pour la catégorie 1:\n",
            "Venezuelans Vote Early in Referendum on Chavez Rule (Reuters) Reuters - Venezuelans turned out early\\and in large numbers on Sunday to vote in a historic referendum\\that will either remove left-wing President Hugo Chavez from\\office or give him a new mandate to govern for the next two\\years.\n",
            "\n",
            "Catégorie avec le plus grand nombre d'exemples : 4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tokenization et construction du vocabulaire\n",
        "\n",
        "Utilisant le tokenizer 'basic_english' de torchtext, nous initialisons un tokenizer et construisons un vocabulaire à partir de l'ensemble de données d'entraînement. Nous définissons une fonction génératrice pour parcourir les exemples, tokeniser le texte et construire le vocabulaire.\n",
        "\n",
        "# Modélisation avec un réseau BERT-like\n",
        "\n",
        "Nous définissons ensuite un modèle de classification de texte BERT-like en utilisant des couches d'attention auto-attentionnelles, des couches feedforward et des connexions résiduelles."
      ],
      "metadata": {
        "id": "k4NIyMkvQnwF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "zqy4mgRBLpgu"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "from torch import nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# Détection de la présence d'un GPU et sélection si disponible pour les calculs, sinon utilisation du CPU.\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Chargement des ensembles de données d'entraînement et de test AG News.\n",
        "train_iter, test_iter = AG_NEWS()\n",
        "\n",
        "# Initialisation d'un tokenizer compatible avec BERT.\n",
        "tokenizer = get_tokenizer('basic_english')\n",
        "\n",
        "# Définition d'une fonction génératrice qui parcourt l'ensemble de données et tokenise le texte.\n",
        "def yield_tokens(data_iter):\n",
        "    for _, text in data_iter:\n",
        "        yield tokenizer(text)\n",
        "\n",
        "# Construction du vocabulaire à partir de l'itérateur de tokens avec un token spécial \"<unk>\".\n",
        "vocab = build_vocab_from_iterator(yield_tokens(train_iter), specials=[\"<unk>\"])\n",
        "vocab.set_default_index(vocab[\"<unk>\"])\n",
        "\n",
        "# Définition d'un modèle BERT-like pour la classification de texte.\n",
        "class SelfAttention(nn.Module):\n",
        "    def __init__(self, embed_dim, num_heads):\n",
        "        super(SelfAttention, self).__init__()\n",
        "        self.embed_dim = embed_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = embed_dim // num_heads\n",
        "\n",
        "        assert (\n",
        "            self.head_dim * num_heads == embed_dim\n",
        "        ), \"La dimension de l'embedding doit être divisible par le nombre de têtes.\"\n",
        "\n",
        "        self.query = nn.Linear(embed_dim, embed_dim)\n",
        "        self.key = nn.Linear(embed_dim, embed_dim)\n",
        "        self.value = nn.Linear(embed_dim, embed_dim)\n",
        "        self.fc_out = nn.Linear(embed_dim, embed_dim)\n",
        "\n",
        "    def forward(self, query, key, value):\n",
        "        N = query.shape[0]\n",
        "\n",
        "        query = self.query(query)\n",
        "        key = self.key(key)\n",
        "        value = self.value(value)\n",
        "\n",
        "        query = query.view(N, -1, self.num_heads, self.head_dim)\n",
        "        key = key.view(N, -1, self.num_heads, self.head_dim)\n",
        "        value = value.view(N, -1, self.num_heads, self.head_dim)\n",
        "\n",
        "        query = query.permute(0, 2, 1, 3)\n",
        "        key = key.permute(0, 2, 1, 3)\n",
        "        value = value.permute(0, 2, 1, 3)\n",
        "\n",
        "        attention_scores = torch.matmul(query, key.permute(0, 1, 3, 2)) / torch.sqrt(\n",
        "            torch.tensor(self.head_dim, dtype=torch.float32)\n",
        "        )\n",
        "\n",
        "        attention_probs = torch.softmax(attention_scores, dim=-1)\n",
        "\n",
        "        attention_probs_dropout = nn.Dropout(p=0.1)(attention_probs)\n",
        "\n",
        "        output = torch.matmul(attention_probs_dropout, value)\n",
        "\n",
        "        output = output.permute(0, 2, 1, 3).contiguous()\n",
        "        output = output.view(N, -1, self.embed_dim)\n",
        "\n",
        "        output = self.fc_out(output)\n",
        "\n",
        "        return output\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, embed_dim, ff_hidden_size):\n",
        "        super(FeedForward, self).__init__()\n",
        "        self.fc1 = nn.Linear(embed_dim, ff_hidden_size)\n",
        "        self.fc2 = nn.Linear(ff_hidden_size, embed_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "class BERTLayer(nn.Module):\n",
        "    def __init__(self, embed_dim, num_heads, ff_hidden_size):\n",
        "        super(BERTLayer, self).__init__()\n",
        "        self.attention = SelfAttention(embed_dim, num_heads)\n",
        "        self.feedforward = FeedForward(embed_dim, ff_hidden_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        attention_output = self.attention(x, x, x)\n",
        "        feedforward_output = self.feedforward(attention_output)\n",
        "        return feedforward_output + x  # Connexion résiduelle\n",
        "\n",
        "class BERTClassifier(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim, num_heads, ff_hidden_size, num_layers, num_classes):\n",
        "        super(BERTClassifier, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
        "        self.layers = nn.ModuleList([BERTLayer(embed_dim, num_heads, ff_hidden_size) for _ in range(num_layers)])\n",
        "        self.fc = nn.Linear(embed_dim, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        embedded = self.embedding(x)\n",
        "        for layer in self.layers:\n",
        "            embedded = layer(embedded)\n",
        "        pooled = embedded.mean(1)  # Pooling moyen sur la dimension de la séquence\n",
        "        output = self.fc(pooled)\n",
        "        return output\n",
        "\n",
        "# Initialisation des paramètres du modèle.\n",
        "vocab_size = len(vocab)\n",
        "embed_dim = 64\n",
        "num_heads = 4\n",
        "ff_hidden_size = 128\n",
        "num_layers = 3\n",
        "num_classes = 4  # Les catégories sont World, Sports, Business, Science/Technology\n",
        "\n",
        "# Création d'une instance du modèle BERTClassifier et déplacement sur le périphérique sélectionné (GPU ou CPU).\n",
        "model = BERTClassifier(vocab_size, embed_dim, num_heads, ff_hidden_size, num_layers, num_classes).to(device)\n",
        "\n",
        "# Définition de la fonction de perte CrossEntropy pour la classification et de l'optimiseur Adam.\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Entraînement et évaluation du modèle\n",
        "\n",
        "Enfin, nous définissons des fonctions de prétraitement des données, une fonction de perte (CrossEntropyLoss), un optimiseur (Adam), ainsi que des fonctions d'entraînement et d'évaluation. Ces fonctions sont utilisées pour entraîner le modèle sur les données d'entraînement et évaluer sa performance sur les données de test."
      ],
      "metadata": {
        "id": "bu-KMdzLQ2Ge"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kJU4_IyGBvDe",
        "outputId": "cff94ac6-5d3b-41d2-ef30-86e5be8b2216"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Train Loss: 0.4249290187701433, Test Loss: 0.043996884397316825, Test Accuracy: 0.8939473684210526\n",
            "Epoch 2, Train Loss: 0.24504690225151923, Test Loss: 0.04088140029958786, Test Accuracy: 0.9078947368421053\n",
            "Epoch 3, Train Loss: 0.1933563622824826, Test Loss: 0.03867199592309541, Test Accuracy: 0.9147368421052632\n",
            "Epoch 4, Train Loss: 0.1557558882748579, Test Loss: 0.04184376310133781, Test Accuracy: 0.9134210526315789\n",
            "Epoch 5, Train Loss: 0.1310835669718252, Test Loss: 0.042738124733468316, Test Accuracy: 0.9118421052631579\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "def collate_batch(batch):\n",
        "    \"\"\"\n",
        "    Prepare a batch for training or evaluation.\n",
        "\n",
        "    Args:\n",
        "    batch (list): A batch of samples from the DataLoader, where each sample is a tuple\n",
        "                  containing the label and the raw text.\n",
        "\n",
        "    Returns:\n",
        "    Tuple[Tensor, Tensor]: The first tensor contains the labels of the batch, and the second tensor\n",
        "                           contains the tokenized and padded texts of the batch, both ready to be\n",
        "                           passed through the model.\n",
        "    \"\"\"\n",
        "    label_list, text_list = [], []\n",
        "\n",
        "    for (_label, _text) in batch:\n",
        "        label_list.append(_label - 1)\n",
        "        processed_text = torch.tensor(tokenize_and_index(_text), dtype=torch.int64)\n",
        "        text_list.append(processed_text)\n",
        "\n",
        "    return torch.tensor(label_list, dtype=torch.int64), pad_sequence(text_list, batch_first=True)\n",
        "\n",
        "def tokenize_and_index(text):\n",
        "    \"\"\"\n",
        "    Tokenize the input text and convert tokens to indices.\n",
        "\n",
        "    Args:\n",
        "    text (str): Raw text to be tokenized and indexed.\n",
        "\n",
        "    Returns:\n",
        "    List[int]: List of token indices representing the input text.\n",
        "    \"\"\"\n",
        "    return vocab(tokenizer(text))\n",
        "\n",
        "def train_model(model, train_loader, test_loader, criterion, optimizer, num_epochs, device):\n",
        "    \"\"\"\n",
        "    Train the LSTM model with the provided data.\n",
        "\n",
        "    Args:\n",
        "    model (torch.nn.Module): The LSTM model to train.\n",
        "    train_loader (DataLoader): Data loader for the training set.\n",
        "    test_loader (DataLoader): Data loader for the test set.\n",
        "    criterion (loss_function): Loss function to use for training.\n",
        "    optimizer (torch.optim): Optimizer to update the model weights.\n",
        "    num_epochs (int): Number of training epochs.\n",
        "    device (torch.device): Device on which the model is run (CPU or GPU).\n",
        "\n",
        "    Returns:\n",
        "    None\n",
        "    \"\"\"\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "        num_batches = 0\n",
        "\n",
        "        for labels, text in train_loader:\n",
        "            num_batches += 1\n",
        "            labels, text = labels.to(device), text.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            output = model(text)\n",
        "            loss = criterion(output, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        train_loss = total_loss / num_batches\n",
        "        test_loss, test_accuracy = evaluate_model(model, test_loader, criterion, device)\n",
        "        print(f'Epoch {epoch+1}, Train Loss: {train_loss}, Test Loss: {test_loss}, Test Accuracy: {test_accuracy}')\n",
        "\n",
        "def evaluate_model(model, test_loader, criterion, device):\n",
        "    \"\"\"\n",
        "    Evaluate the model on the test set.\n",
        "\n",
        "    Args:\n",
        "    model (torch.nn.Module): The LSTM model to evaluate.\n",
        "    test_loader (DataLoader): Data loader for the test set.\n",
        "    criterion (loss_function): Loss function used for evaluation.\n",
        "    device (torch.device): Device on which the model is run (CPU or GPU).\n",
        "\n",
        "    Returns:\n",
        "    float: Average loss on the test set.\n",
        "    float: Model accuracy on the test set.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    correct_preds = 0\n",
        "    total_samples = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for labels, text in test_loader:\n",
        "            labels, text = labels.to(device), text.to(device)\n",
        "            output = model(text)\n",
        "            loss = criterion(output, labels)\n",
        "            total_loss += loss.item()\n",
        "            predictions = output.argmax(1)\n",
        "            correct_preds += (predictions == labels).sum().item()\n",
        "            total_samples += labels.size(0)\n",
        "\n",
        "    return total_loss / total_samples, correct_preds / total_samples\n",
        "\n",
        "# Assuming vocab, tokenizer, and device are defined elsewhere\n",
        "train_loader = DataLoader(train_iter, batch_size=8, shuffle=False, collate_fn=collate_batch)\n",
        "test_loader = DataLoader(test_iter, batch_size=8, shuffle=False, collate_fn=collate_batch)\n",
        "\n",
        "# Assuming model, criterion, optimizer, num_epochs, and device are defined elsewhere\n",
        "num_epochs = 5\n",
        "train_model(model, train_loader, test_loader, criterion, optimizer, num_epochs, device)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Résultats et discussion:\n",
        "\n",
        "Ces résultats d'entraînement et de test fournissent une indication de la performance du modèle de classification de texte BERT-like sur l'ensemble de données AG News.\n",
        "\n",
        "Au cours des cinq époques, nous observons une diminution constante de la perte d'entraînement, passant de 0.42 à 0.13. Cela suggère que le modèle s'améliore progressivement dans sa capacité à prédire correctement les étiquettes des exemples d'entraînement.\n",
        "\n",
        "\n",
        "*   Au cours des cinq époques, nous observons une diminution constante de la perte d'entraînement, passant de 0.42 à 0.13. Cela suggère que le modèle s'améliore progressivement dans sa capacité à prédire correctement les étiquettes des exemples d'entraînement.\n",
        "*   Pour la perte de test, nous constatons une diminution générale de 0.044 à 0.043 sur les cinq époques, indiquant une certaine stabilité dans la performance du modèle sur des données qu'il n'a pas vues auparavant.\n",
        "*   En ce qui concerne la précision du test, nous constatons une augmentation initiale de 0.894 à 0.912 au cours des deux premières époques, ce qui suggère une amélioration notable de la capacité du modèle à généraliser sur de nouvelles données. Cependant, cette précision semble se stabiliser autour de 0.913-0.914 par la suite.\n",
        "\n",
        "\n",
        "Globalement, ces résultats indiquent que le modèle BERT-like a été capable d'apprendre à bien généraliser à partir de l'ensemble de données d'entraînement vers l'ensemble de données de test, atteignant une précision de classification d'environ 91%. Cependant, il semble y avoir une légère tendance à la surajustement, comme en témoigne la légère augmentation de la perte de test après la troisième époque. Ce résultat pourrait nécessiter une exploration plus approfondie pour améliorer la capacité du modèle à généraliser davantage."
      ],
      "metadata": {
        "id": "3wnoLbVBRJHJ"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [],
      "dockerImageVersionId": 30665,
      "isGpuEnabled": true,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}